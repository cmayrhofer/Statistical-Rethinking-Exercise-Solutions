{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAP Text Classification Challenge\n",
    "\n",
    "\n",
    "## Problem statement\n",
    "```\n",
    "Which Novel Do I Belong To?\n",
    "\n",
    "In this task, you are expected to learn a Machine Learning model that classifies a given line as belonging to one of the following 12 novels:\n",
    "\n",
    "0: alice_in_wonderland\n",
    "1: dracula\n",
    "2: dubliners\n",
    "3: great_expectations\n",
    "4: hard_times\n",
    "5: huckleberry_finn\n",
    "6: les_miserable\n",
    "7: moby_dick\n",
    "8: oliver_twist\n",
    "9: peter_pan\n",
    "10: talw_of_two_cities\n",
    "11: tom_sawyer\n",
    "\n",
    "\n",
    "Description:\n",
    "\n",
    "You are provided with a zip file (offline_challenge.zip) containing three text files - xtrain.txt, ytrain.txt, xtest.txt. Each line in xtrain.txt and xtest.txt comes from a different novel. The data has been obfuscated, however the patterns in them are preserved. The novel ids corresponding to xtrain.txt are specified in ytrain.txt. You can use these labels to train a Machine Learning model (Deep Learning preferred).\n",
    "\n",
    "With the learned model, predict the novel ids of the lines in xtest.txt (one prediction per line). As part of your submission, include\n",
    "\n",
    "a) your predictions (in the same format as ytrain.txt)\n",
    "b) Expected accuracy on the test set\n",
    "c) the source code for training and prediction (< 10MB)\n",
    "d) a brief description of your method (optional)\n",
    "```\n",
    "\n",
    "## Summary of My Approach and Result\n",
    "\n",
    "* Insights after EDA of the train set:\n",
    "    - the 12 classes are unequally represented (ratio of biggest to smallest corpus is one to ten)\n",
    "    - length of the character samples is always even\n",
    "    \n",
    "* Before trying out a deep NN approach, we build a simple baseline model.\n",
    "    - As features for the model we used the (scaled) frequency of the characters and the (scaled) character sequence length. (Unfortunately, without obvious word separators, fastText wasn't a viable baseline option)\n",
    "    - Since there wasn't a single odd character sequence, we wondered whether the information is perhaps encoded in character pairs. Therefore, we redid the classification based on frequencies with character pairs. This turned out to be substantially better than on characters, ending up with an accuracy of around 60%.\n",
    "    \n",
    "* Next we turned to deep learning approaches. Since we don't have actual words, it isn't possible to use transfer learning and we have to train our models from scratch. Furthermore, in the deep language model context, I'm only familiar with BERT (which is not applicable here). Hence, the code piece used for the character-level CNN, RNN (LSTM), and Embedding Bag model are modified code snippets from the pytorch docs and github repos.\n",
    "    - Both, character-level CNN and character-level RNN (LSTM), should be able to pick up on the information given in the sentences, even when obfuscated.\n",
    "    - However, due to my unfamiliarity with both I wasn't able to bring them to an satisfactory level. With RNN I ran into the familiar problem with exploding gradients. When changing to LSTMs which solve this problems, I ran into technical issues which I couldn't resolve due to time constraints. In case of CNNs, I only could run them on characters but not character-pairs. The best result which I could reach there was around 35% in accuracy.\n",
    "    - Given the limited time, the Embbeding Bag approach turned out to be the most fruitful one. Although I could reached 55% in accuracy by using character-pair embedding. I wasn't able to beat the 60% of my baseline with `xgboost`.\n",
    "    \n",
    "* To reproduce the results please run the respective cells below\n",
    "\n",
    "### Would-be Next Steps\n",
    "* Get the LSTM to work\n",
    "* Try my own CNN architecture and work with character-pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actual Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 21M\r\n",
      "drwxrwxr-x  2 docker docker 4.0K Sep 24 13:04 .\r\n",
      "drwxrwxr-x 13 docker docker 4.0K Sep 25 07:19 ..\r\n",
      "-rw-rw-r--  1 docker docker    0 Sep 21 14:29 .gitkeep\r\n",
      "-rw-r--r--  1 docker docker 4.3M Sep 24 13:04 train.csv\r\n",
      "-rw-r--r--  1 docker docker 2.1M Sep 24 13:04 valid.csv\r\n",
      "-rw-r--r--  1 docker docker 1.2M Jul 19  2017 xtest_obfuscated.txt\r\n",
      "-rw-r--r--  1 docker docker  13M Jul 19  2017 xtrain_obfuscated.txt\r\n",
      "-rw-r--r--  1 docker docker  68K Jul 19  2017 ytrain.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lha ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ../data/train.csv <==\r\n",
      "0,enuhtwskenezuhsaiwulvienulqvvimvuhpmamdfuhqgsalekrbruhtwamuluhpmuluhlrvimvvilekruluhqgsktwezmveniwypcitwuhlrvimvtwuhtwamuluhijoavitwiwpmulxykguhskvienuhqggzkruheztwamuluhsktwskskenuhvitwamuluhskvienuh\r\n",
      "6,ulenamuluhezmvamuhpmuluhsktwypvikrpmvivienuhtwleulmvpmvivipmyptwleuhpmuluhsktwamypulengzqvtwskuhvitwtwtvuhskenamuhpmamdfuhqvletwypmvxexepmuhpkvipmmvqvkriwiwmvuhtwvipmuhulqvpmiwuhtwamuhqglrvimvvilepmez\r\n",
      "3,satvuhskiwtwamuhsaiwtwiwpmqvuhqvmvuhultwleyptwqvuhtwamuluhskvienuhqgqvvipmmvulenulyptwgzcitwuhlekrpmsauhtwmkmvletwskuhkrpmsauhezpmamdfuhezpmlexeuhvipmqvletwgzuhtwamuluhqvmvuhvipmqvletwgzuhulenamguuhra\r\n",
      "8,twamuluhvwskiwkrpmypvwuhulenamokuhuhrasaleenvimvsklepmenleulcitwuhsaletwmkuhqvendfuhulmvuhtwlekrqvuhtwtvuhpmuluhskviskuhuhraulenpmypulqvmvendfuhtwulmvamdfuhtwamuluhvimvuhvieneztwiwulvitwlruhtwamuluhsk\r\n",
      "10,letwamuhvienamuluhiwenkrulmvlemvgzqvuhqvletwululenezuhpmuluhqvenuhtwiwpmamdfuhtwamuluhvipmuhsktwlekrpmmkenxeuhqvqvtwiwuhqgtwypvienleuyuhtwvieniwbhucyppmeeuhtwamuluhxepmuhqvvitwucypmvamypuhtwamuluhxepm\r\n",
      "\r\n",
      "==> ../data/valid.csv <==\r\n",
      "7,ohskmvenqvuhtwamuhulenamuluhqvijulenamokuhraqvletwamulenxeuhiwenulmvmkuhxepmuhxexekrulqvuhiwenulmvmkuhulqvpmezuhtwamuluhxepmuhtwskenezuhvitwezuhxepmuhqvtwmkmviwuhtwamuluhulqveniwulkrpmuhqvtwtwleuluhsk\r\n",
      "10,skvienuhqgulmvuhxexepmuhulqvkrskuhtwamuluhlrvimvdfpmiwtvuhskvienuhqgxetwmvamypletwucskvienamuhletwamuhxepmuhqvskvitwuhtwamuluhamulmvdfuhgzkruhulmvuhlrvimvamypkrpmuluhskvienuhqgqvlemvenulqvbhgzkruhtwez\r\n",
      "10,twamuhtwiwmvamdfuhsaiwvipmuhqgskvienamuhulxetwiwuhqvmvamuhamulmvdfuhletwiwxexekrezuhqvmvamuluhsktwmkpmezuhtwamuhqgucvimvleskuhlepmxeuhsktwgzgzpmulqvuhtwamuhvitwamokuhraqvtwtwviucuhqvijletwleentwdfuhtw\r\n",
      "6,ulpmviuhqvendfuhamypmvamdfuhqgleentwuluhenuhlrvikramuhamypmvamdfuhvimvuhqgqvtwsatwuhulentwlelruhletwamuhsktwqvmvenleuhskiwmvamypuhtwamguuhdsraqvtwmkiwtwqveztwamuluhtwqvkrezenuhsatwamuluhpksaeniwgzuhsa\r\n",
      "1,pmuluhqvenuhpmqvuhpksaypviensapmkrtvuhskiwpmuhqvmvamuhxepmuhlrvimvamultwezpmqvuhletwmkpmyptwleuhpmuluhlrvimvvivimvlrtwtvuhqvmvuhlekramulleskuhulenamuluhleentwamuhiguhezmvamuhezpmlexeuhskvienuhqgpmpmul\r\n",
      "\r\n",
      "==> ../data/xtest_obfuscated.txt <==\r\n",
      "tvletwgzkrqvuhtwamuluhpkskpmpmiwtvuhamqvmviwlrvikquhtwamuluhqgvipmmvulkriwpmqvtwleuhamqvmviwlrvikquhtwamuluhqgqvqvtwviezlemvxeuhamqvmviwlrvikquhtwamuluhpkskvieniwlrvikquhqvmvuhqgpmpmiwletwulenokuhxepmuhtwiwululentvuhtwamuluhvimvuhsktwlemvezskenuhtwtvuhulqvkrezuhamypmvamdfuhulenamguuhraskvipmyptwqvuhtwamuluhxepmuhvimvenulgzenypuhenuhsatvuhvipmdfuhqgletwsklepmuhulqvlemvxeuhtwamuluhxepmuhtwiwululentvuhenuhqvmvuhpmpmiwletwulenok\r\n",
      "qvmvuhskleenmvviengzxyuhqvmvamguuhrakrpmsauhulyptwulpmlegzuhskigbhbhkrpmsauhulyptwulpmlegzuhskigbhbhpmfquhraskiwlepmdfuhtwamuluhiwiwenuhlepmxeuhskentwamuhlekrpmsauhxepmuhlemvenamuhenuhullekramuhulviskiwkrpmdfuhiguhratwezuhxepmuhskmvenlexeenuhtwtvuhulvipmskuhqgsapmtvuhsaqjuhraletwskvikriwtvuhletwulxeenuhletwskvikriwtvuhtwskenezuhtwamuhskvienuhezmvamuhvipmgzkruhqvendfuhtwsatwuhqvvienezuhskiwpmuhtwamuluhulkrtv\r\n",
      "twamuhulenamuluhskvienuhqgdftwmvmkletwulvimvuhvienuhqvtwypvitwgzcimvqvuhvitwtwulxemvxeuhulenuhgzentwamypuhtwypvienulvimvenkrszypenuhqvmvamuhsktwletwskmvqvvipmypuhtwamuhulenamuluhsaviengzezpmypuhtwamuluhsktwlekrqvqvenuhqgulamlrmvqvuhxepmuhulkrpmuhtwletwdfuhsatwamuluhiwiwmvuluhqvulpmpmtvuhskvienuhqvlrtwiwuhqvmvamuhvipmuhqvtwypvieniwlruhlrvimvlemvezskenuhqvkrpmletwezkrviuhlrvimvdfpmulqvtwtvuhqglrvimviwulmvameeuhraleqj\r\n",
      "sooatwqvvitwqvvipmviuhoaampmpmmwuhuhraulvikrpmypypenuhulenamuluhvipmuhsaypletwezuhpmuluhsktwskvitwezezpmyptwleuhulsoviqvendfuhtwamuhpkqvtwezmvuluhcimvqvuhletwmktwxeuhenuhskenamuhskenamuhtwlpuhuhraletwulqvenezuhqvmvamuhlrvimvletwsklekrezuhlepmxeuhenypmvenezencguhvimvuhlrvikramuhqvendfuhpmamdfuhvienezuhenuhdftwviucuhiguhuhohamtwuhqgsatwamuluhulsovitwmkenamuhpkqvtwezmvultwezpmqvuhqvletwmktwxe\r\n",
      "lepmuhpmdfuluhtwletwdfuhtwletwamguuhuhraezpmpmleuhtwamuluhpmulvimvuhezmvamuhskvienuhqvtwmkiwtwqveztwamuluhsktwulenkrvimvqvvimvuhsaiwiwenkrskenlelruhqgqvletwskiwmvameeuhraleqjuhpmuluhskvienuhletwamulpmvienuhtwvipmuhpmuluhqvtwypmvpmmkuhdfpmiwuhvimvuhlrvimvuciwenuluhqgulkrpmtvenuhlrvimvskvienulqvuhezpmlexeuhqgskvienuhqgsktwletwulleenkrszuhtwletwdfuhsatwamuluhtwletwamdfuhqgqvvipmmvlrtwleuhletwgzgzkruhtwamuluhezpmlexeuhletwamultwlrpmul\r\n",
      "\r\n",
      "==> ../data/xtrain_obfuscated.txt <==\r\n",
      "satwamuluhqgulamlrmvezuhqvkrpmletwulcitwskuhlemvtwamuluhiwiwenuhlrvimvqvkruhulenamuluhqgqvtwvimviwuhtwamuluhulqvkrenamcitwuhvipmpmqvuhskiwkrpmdfuhlrvimvskvikrpmqvuhskmvgzenleuhqvmvamuluhulenamuluhqvletwtwvipmpmgzleenamuhtwamuluhtwletwdfuhiwkrxeleentwxeuhpmqvuhtwiwmvamdfuhpkeztwamuluhvimvuhqvtwmkpmpmlelruhgztwtwskuhtwlrkrpmlruhpmuluhqvenuhtwyplepmxeuhenuhamypkrqvuhamulmvdfuhqvskentwamletwlrlrpmiwuhtwamul\r\n",
      "twmkiwpmqvtwleuhsaiwsktwmvlelekramuhqvkruhtwskenezuhskvienuhqgulvienulqvvimvuhvienuhvimvuhulyptwbrtvkrqvuhtwamuluhsktwlrvienamypuhqvmvamguuhvgoaulamlrmvvibhpmuluhqvijulmvnkuhqgskkrpmiwenuhsktwskskenuhsaiwmvleenulvikriwpmmkvimvuhiguhvgqgulleentwamuhsaezuhqvqvtwiwtvuhskvisknkuhravidfpmvitwleuhvienmvypqvpmjeuhxepmuhlekrtwulenezenuhiwenmvypvimvmkpmlegzuhsktwulenletvtwiwtwypuhtwamuluhpmul\r\n",
      "vidfpmskuhvilepmuluhtwtvuhulsovienamqvuhskiwmvamypuhtwamulsouhqgvienezuhtwamuluhamulmvdfuhsaiwulvitwiwpmmvmkuhlrvimviwlrlrkrleulqvuhqgiwlemvlruhtwamuluhsktwezentwleypqvuhsoqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhiguhqgiwiwmvdfuhqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhigsouhulqvvimvenlrenuhskentwamuhlekrpmsauhulmviwgzqvuhiwiwsoiguhlepmuhqgtwezuhezpmlexeuhxexepmuhskvienulxysouhuhragzqvenlelruhqvsoiwlemvlruhtwamul\r\n",
      "raskleenkrlruhtwulenleengztwqvuhenuhsatvuhsktwskvikrpmlelekrqvuhsktwuciwendfuhamypentwuhskvienuhqgulleengzenuhulgztwucuhtwletwdfuhpmdfuluhtwamguuhralrvimvezpmypuhtwleenuhkrpmsauhdfpmviucuhsatwamulnkuhpkulypmvmkvipmypuhsaezuhpmuluhulvientwlrletwqvuhtwamuluhskmvenqvuhvgqgskleenpmtvuhvipmuhsktwulyptwgzcitwuhtwleenuhkrpmrbnkuhraleentwuhsaezuhtwskmvqvvimvuhlrvimvamultwezpmqvuhulqvlekrtvuhpmuluhsktweztwtwqvuhulenamuluhskvikrpmqv\r\n",
      "dfenqvuhtwamqvuhqgtwiwtvenuluhqvijletwamulenxeuhletwamuhleentwviuhvidfpmskuhulenqvuhtwamqvuhvitwamdfuhskvienuhpkvidfpmultwucpmeeuhqvskleendfpmuluhsktwucpmpmiwuhdfpmskvimvdfuhskuhuhralrvimvvilepmezuhsktwulvimvpmgzgzenuhtwamuluhvipmuhsktwlemvengztwleuhenqvmvkrpmxzuhpkskmviwbhvimvxexepmypuhenuhvipmgzkruhgzenleuhenuhtwucmviwuhulentwtvuhenuhamulmvdfuhskvipmyptwqvuhsaletwmktwuhsktwlekrqventwezuhamypmvamdfuhqgulmvuhvimv\r\n",
      "\r\n",
      "==> ../data/ytrain.txt <==\r\n",
      "7\r\n",
      "3\r\n",
      "8\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ../data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/xtrain_obfuscated.txt', header=None)[0].values\n",
    "train_labels = pd.read_csv('../data/ytrain.txt', header=None)[0].values\n",
    "\n",
    "test_data = pd.read_csv('../data/xtest_obfuscated.txt', header=None)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['satwamuluhqgulamlrmvezuhqvkrpmletwulcitwskuhlemvtwamuluhiwiwenuhlrvimvqvkruhulenamuluhqgqvtwvimviwuhtwamuluhulqvkrenamcitwuhvipmpmqvuhskiwkrpmdfuhlrvimvskvikrpmqvuhskmvgzenleuhqvmvamuluhulenamuluhqvletwtwvipmpmgzleenamuhtwamuluhtwletwdfuhiwkrxeleentwxeuhpmqvuhtwiwmvamdfuhpkeztwamuluhvimvuhqvtwmkpmpmlelruhgztwtwskuhtwlrkrpmlruhpmuluhqvenuhtwyplepmxeuhenuhamypkrqvuhamulmvdfuhqvskentwamletwlrlrpmiwuhtwamul',\n",
       "       'twmkiwpmqvtwleuhsaiwsktwmvlelekramuhqvkruhtwskenezuhskvienuhqgulvienulqvvimvuhvienuhvimvuhulyptwbrtvkrqvuhtwamuluhsktwlrvienamypuhqvmvamguuhvgoaulamlrmvvibhpmuluhqvijulmvnkuhqgskkrpmiwenuhsktwskskenuhsaiwmvleenulvikriwpmmkvimvuhiguhvgqgulleentwamuhsaezuhqvqvtwiwtvuhskvisknkuhravidfpmvitwleuhvienmvypqvpmjeuhxepmuhlekrtwulenezenuhiwenmvypvimvmkpmlegzuhsktwulenletvtwiwtwypuhtwamuluhpmul',\n",
       "       'vidfpmskuhvilepmuluhtwtvuhulsovienamqvuhskiwmvamypuhtwamulsouhqgvienezuhtwamuluhamulmvdfuhsaiwulvitwiwpmmvmkuhlrvimviwlrlrkrleulqvuhqgiwlemvlruhtwamuluhsktwezentwleypqvuhsoqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhiguhqgiwiwmvdfuhqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhigsouhulqvvimvenlrenuhskentwamuhlekrpmsauhulmviwgzqvuhiwiwsoiguhlepmuhqgtwezuhezpmlexeuhxexepmuhskvienulxysouhuhragzqvenlelruhqvsoiwlemvlruhtwamul',\n",
       "       ...,\n",
       "       'qvmvamuhleentwtvuhpmuluhskvienuhqgtwxemviwuhqvijvitwonmvulmvypuhenuhtwmkenqvuhpmuluhqgucypentvuhtwezpmypuhskenamuhtwlpuhratwypvitwqvtvenuhqvmvamuhsatvuhsktwletwlrvienskvitwuhqvendfuhtwxemviwuhqvmvamuhulenamuluhsktwulvitwqvtwlegztwleuhpmamdfuhqgvitwonmvulmvypuhamypvitwleuyuhenuhxepmuhsaulentwleulvitwuhvitwululmvledfuhskvienuhlrvimvqvqvtwlegzuhtwamuluhvipmuhqgskmvskuhtwamuhvitwamdfuhsktwvilekrultwleuhskenamuhtwlp',\n",
       "       'ralrvimvamypenpmlegzgzenuhqvendfuhgztwulqvuhtwamguuhralrvimvvitwmktwuhlrvimvsktwyptwlegzuhtwamuluhvipmuhqvlemvenulqvuhtwamuluhsktwskvitwypqvenuhskenamuhamypmvamdfuhgztwulqvulpmpmxeuhtwvimviwkrypqvenezuhtwezenqvuhtwamuluhskleentwamuhtwamuhvitwamuluhqglepmskmvlelepmypuhtwamuluhxepmuhskvitwuhtwamuluhulenuhypmvululenuhtwezpmqvuhvipmuhsktwvitwgzpmuhamypmvamdfuhlepmpmskuhenuhxepmuhlrvimvucentwleypuhtwamuluhsatvuhsktwvitwucendfen',\n",
       "       'twiwtvenvipmmvulqvtwkrszuhqvmvamuhsktwmktwmvamypenuhskenamuhtwiwqvgzpmokuhraleqjuhtwletwamdfuhtwleulentwamuluhtwamguuhrasaeniwgzuhtwamuluhpmuluhpmlruhqvskleendfletwulxeenuhskiwkrpmdfuhiguhulamlrkrpmamuluhiguhqgtwiwgzeztwguuhtwamuluhpmuluhtwezpmamuhulvitwdfuhiguhxemvuhtwezuhtwlepmxetwtvuhtwskkrulmviwpmqvuhskvienuhvipmmvulyptwbrtwskuhxepmuhqvlekrpmamuhskenamuhiguhqvenuhskvienuhpktwypvipmuhulenuhletwvivimvskuhamulmvdfuhxeiwtwqvsaez'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  3,  8, ..., 10,  6,  3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 543,\n",
       " 1: 3459,\n",
       " 2: 1471,\n",
       " 3: 4023,\n",
       " 4: 2337,\n",
       " 5: 2283,\n",
       " 6: 4226,\n",
       " 7: 5097,\n",
       " 8: 3634,\n",
       " 9: 980,\n",
       " 10: 3052,\n",
       " 11: 1408}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distribution = dict(zip(*np.unique(train_labels, return_counts=True)))\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.gcd.reduce([len(x) for x in np.hstack((train_data, test_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 35513\n",
       "unique                                                35513\n",
       "top       qglrvimvlemvgzqvenuhamlrmvamuhqgamyvuhoaletwdf...\n",
       "freq                                                      1\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(np.hstack((train_data, test_data)))\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    32513.000000\n",
       "mean       413.389352\n",
       "std         15.017002\n",
       "min        168.000000\n",
       "25%        408.000000\n",
       "50%        416.000000\n",
       "75%        424.000000\n",
       "max        452.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series([len(x) for x in train_data])\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = s.describe()['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean      413.350667\n",
       "std        16.367315\n",
       "min        72.000000\n",
       "25%       408.000000\n",
       "50%       416.000000\n",
       "75%       424.000000\n",
       "max       448.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series([len(x) for x in test_data])\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Model / Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def character_count(s: string) -> list:\n",
    "    count = Counter(s)\n",
    "    return [count[l] for l in string.ascii_lowercase[:26]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cc = np.array(\n",
    "    [character_count(datum) for datum in train_data]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cc_scaled = np.multiply(\n",
    "    1 / np.sum(train_data_cc, axis=1), train_data_cc.T\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cc_scaled = np.vstack(\n",
    "    (\n",
    "        train_data_cc_scaled.T,\n",
    "        np.sum(train_data_cc, axis=1) / max_sentence_length\n",
    "    )\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([26.81287885, 27.2951386 , 27.08031058, 28.42026544, 46.02161407,\n",
      "       32.88086009, 32.33731484, 30.79939771, 33.74606967, 31.8843894 ]), 'score_time': array([0.12062263, 0.11717463, 0.11736608, 0.20314169, 0.12583208,\n",
      "       0.13158917, 0.13236332, 0.13455319, 0.12441754, 0.13173556]), 'test_precision_macro': array([0.401438  , 0.38090671, 0.3745211 , 0.36786108, 0.38957467,\n",
      "       0.37361898, 0.36451369, 0.36045979, 0.35139303, 0.36840286]), 'test_recall_macro': array([0.33451716, 0.33583023, 0.32377269, 0.33253618, 0.32658816,\n",
      "       0.33290097, 0.32300471, 0.32657898, 0.30980125, 0.3236686 ]), 'test_f1_macro': array([0.34559895, 0.34559668, 0.33396055, 0.33585365, 0.33733372,\n",
      "       0.34033878, 0.33013718, 0.33103353, 0.31754631, 0.33417276]), 'test_accuracy': array([0.39298893, 0.39421894, 0.37423124, 0.3949554 , 0.37988311,\n",
      "       0.39218702, 0.38295909, 0.38726546, 0.37096278, 0.37373116])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "priors = [\n",
    "    class_distribution[i] / sum(class_distribution.values())\n",
    "    for i in range(len(class_distribution))\n",
    "]\n",
    "\n",
    "# clf = GaussianNB(priors=priors)\n",
    "# clf = ComplementNB(class_prior=priors)\n",
    "# clf = MultinomialNB(class_prior=priors)\n",
    "# clf = AdaBoostClassifier()\n",
    "# clf = GradientBoostingClassifier()\n",
    "clf = XGBClassifier()\n",
    "\n",
    "scores = cross_validate(\n",
    "    clf, train_data_cc_scaled, train_labels, cv=10,\n",
    "    scoring=['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']\n",
    ")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Model / Baseline Model but now taking GCD being 2 into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_count(s: string) -> list:\n",
    "    pairs = [s[2 * i: 2 * (i + 1)] for i in range(len(s)//2)]\n",
    "    return Counter(pairs)\n",
    "\n",
    "def pair_count_ordered(s: string, unique_pairs: list) -> list:\n",
    "    pair_counts = pair_count(s)\n",
    "    return [pair_counts[pair] for pair in unique_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ku': 2, 'fd': 1, 'sl': 1, 'iu': 1, 'hf': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_count('kufdsliuhfku')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique pairs in train set: 105\n",
      "Number of unique pairs in total set: 105\n"
     ]
    }
   ],
   "source": [
    "total_pairs = Counter()\n",
    "for datum in train_data:\n",
    "    total_pairs.update(pair_count(datum))\n",
    "\n",
    "print(f\"Number of unique pairs in train set: {len(total_pairs)}\")\n",
    "\n",
    "for datum in test_data:\n",
    "    total_pairs.update(pair_count(datum))\n",
    "\n",
    "print(f\"Number of unique pairs in total set: {len(total_pairs)}\")\n",
    "\n",
    "unique_pairs = list(total_pairs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_frequency_feature_preparator(data, max_length=None):\n",
    "    data_pc = np.array(\n",
    "        [pair_count_ordered(datum, unique_pairs) for datum in data]\n",
    "    )\n",
    "    data_pc_scaled = np.multiply(\n",
    "        1 / np.sum(data_pc, axis=1), data_pc.T\n",
    "    ).T\n",
    "    if max_length is not None:\n",
    "        data_pc_scaled = np.vstack(\n",
    "            (\n",
    "                data_pc_scaled.T,\n",
    "                np.sum(data_pc, axis=1) / max_length\n",
    "            )\n",
    "        ).T\n",
    "    return data_pc_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pc_scaled = pair_frequency_feature_preparator(\n",
    "    train_data, max_length=max_sentence_length // 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([66.66461778, 70.9385922 , 69.14417005, 73.05050969, 69.74076605]), 'score_time': array([0.33570361, 0.28826284, 0.26490188, 0.26058269, 0.25916624]), 'test_precision_macro': array([0.615627  , 0.62139878, 0.62072327, 0.61413382, 0.60002419]), 'test_recall_macro': array([0.55343775, 0.55161525, 0.5516301 , 0.55481012, 0.54073931]), 'test_f1_macro': array([0.57446989, 0.57473951, 0.57464226, 0.57597649, 0.56105994]), 'test_accuracy': array([0.59956943, 0.5948024 , 0.59249577, 0.59535528, 0.58781913])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "priors = [\n",
    "    class_distribution[i] / sum(class_distribution.values())\n",
    "    for i in range(len(class_distribution))\n",
    "]\n",
    "\n",
    "# clf = GaussianNB(priors=priors)\n",
    "# clf = ComplementNB(class_prior=priors)\n",
    "# clf = MultinomialNB(class_prior=priors)\n",
    "# clf = AdaBoostClassifier()\n",
    "# clf = GradientBoostingClassifier()\n",
    "clf = XGBClassifier()\n",
    "\n",
    "scores = cross_validate(\n",
    "    clf, train_data_pc_scaled, train_labels, cv=5,\n",
    "    scoring=['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']\n",
    ")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 11, 10, ...,  3,  7,  6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = XGBClassifier()\n",
    "classifier.fit(train_data_pc_scaled, train_labels)\n",
    "text_data_pc_scaled = pair_frequency_feature_preparator(\n",
    "    test_data, max_length=max_sentence_length // 2\n",
    ")\n",
    "classifier.predict(text_data_pc_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {'lables': classifier.predict(text_data_pc_scaled).astype(str)}\n",
    ").to_csv(\"../data/ytest.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\r\n",
      "11\r\n",
      "10\r\n",
      "8\r\n"
     ]
    }
   ],
   "source": [
    "!head -n4 ../data/ytest.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level RNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([5, 1, 26])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "all_letters = string.ascii_lowercase[:26]\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('j'))\n",
    "\n",
    "print(lineToTensor('jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([7, 1, 105])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_pairs = len(unique_pairs)\n",
    "\n",
    "\n",
    "def lineToPairs(line):\n",
    "    pairs = [''.join((line[2 * i],  line[2 * i + 1])) for i in range(len(line)//2)]\n",
    "    return pairs\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def pairToIndex(pair):\n",
    "    return unique_pairs.index(pair)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def pairToTensor(pair):\n",
    "    tensor = torch.zeros(1, n_pairs)\n",
    "    tensor[0][pairToIndex(pair)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line) // 2, 1, n_pairs)\n",
    "    for li, pair in enumerate(lineToPairs(line)):\n",
    "        tensor[li][0][pairToIndex(pair)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(pairToTensor('xe'))\n",
    "\n",
    "print(lineToTensor('xepmuhlekrezlek').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "n_categories = len(class_distribution)\n",
    "rnn = RNN(n_pairs, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "tensor([[-2.5522, -2.4131, -2.5307, -2.4136, -2.4937, -2.4842, -2.3735, -2.5097,\n",
      "         -2.5134, -2.4911, -2.5070, -2.5551]], grad_fn=<LogSoftmaxBackward>)\n",
      "and hidden state:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "input = pairToTensor('xe')\n",
    "hidden =torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "print(f\"output:\\n{output}\\nand hidden state:\\n{hidden}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5522, -2.4131, -2.5307, -2.4136, -2.4937, -2.4842, -2.3735, -2.5097,\n",
      "         -2.5134, -2.4911, -2.5070, -2.5551]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = lineToTensor('xepmuhlekrezlek')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict = {\n",
    "    0: \"alice_in_wonderland\",\n",
    "    1: \"dracula\",\n",
    "    2: \"dubliners\",\n",
    "    3: \"great_expectations\",\n",
    "    4: \"hard_times\",\n",
    "    5: \"huckleberry_finn\",\n",
    "    6: \"les_miserable\",\n",
    "    7: \"moby_dick\",\n",
    "    8: \"oliver_twist\",\n",
    "    9: \"peter_pan\",\n",
    "    10: \"tale_of_two_cities\",\n",
    "    11: \"tom_sawyer\",\n",
    "}\n",
    "\n",
    "all_categories = list(categories_dict.values())\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_line_length = 200\n",
    "X = np.array([line[:max_line_length] for line in train_data])\n",
    "y = train_labels\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "category_lines = dict()\n",
    "for key, value in categories_dict.items():\n",
    "    category_lines[value] = X_train[y_train == key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('les_miserable', 6)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = peter_pan / line = satvuhiwentwulqvuhsatwamuluhqvenuheztwamuluhamypulendfuhskvienuhtwvienypbhleenlrkrqvuhtwamuluhlrvipmezenuhtwletwamuhtwmviwuhpmuluhskvitwultwlegzuhqvkruhultwxzuhrapmdfuluhqvenuhqvvimvdfuluhtwamuluhlrvi\n",
      "category = alice_in_wonderland / line = saiwiwenletwvitwlruhtwamxyuhijoatwulkrvimvezuhqvmvamuluhxexepmuhtwmkentwiwuhpmuluhkrpmsauhtwqvmvmkskenuhigkguhpksaiwgzleenamqvuhletwamulenleuhqgxeiwtwqvletwamuhpmuluhtwypmviwskuhskmvenqvuhijoaulenamul\n",
      "category = oliver_twist / line = qvsodfpmiwvidfpmledfuhraleqjuhamypmvamdfuhpmulvimvuhulmvxebhlrvimvulvimvenxeuhtwamuluhezpmlexeuhlrvimvletwmkpmyptwleuhvipmpmqvuhletwmkmviwyvuhqgqjiglpuhprfqigfqjekqeefqyveeuhmckqjekqguguskuhprigokqjig\n",
      "category = tale_of_two_cities / line = letwamuhqgxeiwtwqvletwamuhskvienuhqgletwamulenxeuhletwamuhlekrtwmvqvvipmqjuhlepmxeuhsaamulengzezsaqvuhlekrpmuhiwiwenuhletwulxeenuhqgulenamulnkuhqgulmvuhulmvezskenuhpmuluhletwamuhtwypkrskvimvuhpmuluhtw\n",
      "category = les_miserable / line = letwmkpmuhtwvipmlruhskenamuhpmamdfuhvitwezuhpksktwvipmululkrtvvikruhqvtwmkiwtwqveztwamuluhsktwdfpmamqvuhqvlepmulmvenleguuhravipmqvmvlegzuhvimvuhqvamulvipmezuhcimvxyuhratwypvienletwululkruhqvkrpmmvulmv\n",
      "category = les_miserable / line = qvmvuhulenamokuhoaqvleenulqvuhqvqvtwiwtwqvkruhqgqvulqvkrlruhqgqvskvimvdfiwlemvamdfuhqgqvskkrpmiwypuhqgqvskvimvokuhraqvqvtwvilrvimvamulpmviuhgzqvenlelruhskvienuhqgtwqvpmiwypuhsatwamuluhpksaiwtwmkmvqviw\n",
      "category = les_miserable / line = saezuhxepmuhqvskvikrpmtvuhtwamuluhvimvamulmvdfuhezenuhiguhulenamuluhucleeneztwleuhpmuluhtwezuhulmvezletwgzuhiwiwmvdfuhtwamuhulkrtvuhqgtwxemviwuhsaezuhvimvuhtwezmvuluhulqvlemvxeuhtwamuluhlepmxeuhqvmvuh\n",
      "category = dracula / line = qvenuhezleenuhsaezuhsktwamypkrpmuluhqvvipmmvviengzezpmypuhsaezuhxepmuhtwviyvuhraletwulendfuhlrvimviwiwenxeuhxepmuhezentwiwlruhtwulmvamdfuhtwamuluhvimvenlrenuhskvienuhdfpmviuhdfenqvuhtwdfuhqgucvimvqvuh\n",
      "category = great_expectations / line = ralrvimvpmskuhsaezuhiwiwenuhqvendfuhulmvuhucvimvamuluhskiwkrpmdfuhsatwamuluhulenamuluhulleengzuhsaezuhvipmuhqvqvtwviqvkrpmmvypqvvipmypuhsktwypenxebhtwezenamqvuhenuhamulmvdfbhbhqgvipmuhsktwulqvpmgzuhtw\n",
      "category = peter_pan / line = qvqvtwiwvikruhsaiwxeuhvienypuhtwvipmuhpmviuhlepmxeuhqgeztwamuluhamulmvdfuhlrvimviwxemvleuluhvitwtwtvuhskenamuhletwultwmwuhtwqvlekrpmypuhxeyvuhulpmviuhskmvskuhletwultwmwuhskvienuhqgqvtwiwtveniwiwsaqvuh\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_value_(rnn.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 1% (2m 37s) 2.5166 twletwamuluhulkrdfuhraulvitwmktwuhskenqvuhtwamuluhulkrpmtvenuhqvletwgzqvmvamdfuhvimvuhtwqvletwmkvipmypuhpmuluhtwiwkrtvmvulqvtwmkuhtwamuluhvimvuhulvitwezpmezuhenuhlrvimvletwulmvpmiwuhqgletwamulenlruhpm / moby_dick ✗ (tom_sawyer)\n",
      "10000 3% (4m 49s) 2.3314 raezmvamuhlrvimvsaxemvulvitwskmvuhxepmuhtwqvpmgzlekrgzuhtwamuluhlepmxeuhtwqvkrpmamuhsaleulvikrpmypuhtwamuluhpmuluhulmvqvmvmkuhlemvtwamuluhpmuluhsktwiwuhskenamuhskentwamuhqvmvamuluhvipmuhtwulkrgzqvmvsk / hard_times ✗ (oliver_twist)\n",
      "15000 5% (7m 9s) 2.4852 twiwenamdfuhulqvlemvxeuhtwamuluhulenamuluhqgskmvenqvuhulmvuhtwtvuhlrvimviwiwenypuhlekrpmuhxepmuhlekrpmvipmamuhiwenviletwultwuhtwamuluhpmuluhskvienuhpkvieneztwiwenamdfuhulqvlemvxeuhtwamuluhqvendfuhqgle / tale_of_two_cities ✗ (moby_dick)\n",
      "20000 6% (9m 29s) 2.8216 qvsklemvtvuhtwamuluhpmuluhuciwenuluhvienypuhtwiwgzpmtwgzuhqvtwmvlepmulqvuhiwkrxemvypvienxeuhviiguhratwlrenkrlrvieniwuhqvijletwamulpmuhtwamuluhskpmpmulqvletwskvikruheztwamuluhxepmuhletwamulmvtwviuhtwqv / alice_in_wonderland ✗ (peter_pan)\n",
      "25000 8% (11m 42s) 2.0526 letwamuhskiwpmuluhtwamqvuhskvienuhqgtwypmviwskuhskmvenqvuhijoaezentwleskuhqvkrpmmvlekrypuhenuhamypkrqvuhskenamuhtwmkijiguhqgamyvkguhtwmkijkrpmsauhgztwtwiwqvuhlrvipmiwuhenuhulenamdfuhqgsaamokkguhpkletw / alice_in_wonderland ✓\n",
      "30000 10% (14m 8s) 2.4694 twtvuhpmuluhvipmmvulmvskvipmypuhpmviuhvimvuhqvmvuhskvienuhqgezmvamuhvipmgzkruhletwmktwxeuhxepmuhqvezpmulgzezsaqvuhlrvipmleulqvuhqvenamuhsapmtvuhtwamguuhuhratwqvkrypcitwuhlekrpmuhtwtvuhulqvkrezuhulenam / peter_pan ✗ (oliver_twist)\n",
      "35000 11% (16m 23s) 2.6267 twypeniwgzuhletwamuhskvienuhpkqvgzenamletwgzuhqgiwpmpmxeuhenuhqvendfuhtwamqvuhsaenxyuhohdfpmiwiwtwxeuhenuhamulmvdfuhsaendfenuhvikrleuhskenamuhtwamqvuhulenamuluhulkrpmuhskvikrpmxeuhsatwamuluhvitwamdfuh / peter_pan ✗ (dubliners)\n",
      "40000 13% (18m 45s) 2.4062 qvmvamuhamulmvdfuhulqveniwuhlrvimvletwulvitwuhqgsatvletwskvikrpmdfuhraleqjuhskmvenqvuhijoaamtwuhqgqviwmvpmuhtwvimvviuhskvienuhqvlrtwiwsaleletwqjkguhijraqvucleentvuhsaiwvipmuhtwamuhskvienuhqgqvlrtwiwsa / huckleberry_finn ✗ (hard_times)\n",
      "45000 15% (21m 3s) 2.3183 ratwqvvitwgzqvkrqvuhxepmuhlrvimviwtwtwxeuhucypmvqvuhenuhamulmvdfuhsktwulmvendfuhiguhraqvtwypvitwmvletwgzcitwuhulvitwyptwleuhsaezuhsatvuhsktwqventwleypvimvuhqvendfuhulamlrmvviskmvezuhulkrpmtvenuhvipmmv / peter_pan ✗ (dracula)\n",
      "50000 16% (72m 54s) 2.3193 xepmuhulvimvezuhenuhqvulqvpmypuhuliguhuhrasatwvipmezuhqvulqvpmypuhuliguhuhraiwiwenuhulenuhkrpmsauhlepmxeuhulpmviuhqvijulmvuhulkrdfkguhqvmvamuhvimvuhlrvimvletwskviendfuhletwmktwviuhskvienuhqgletwvivien / tom_sawyer ✗ (hard_times)\n",
      "55000 18% (75m 0s) 2.5886 qvqvtwiwuhlepmuhtwlepmezuhqgvitwleskiwmvamypuhulkrpmtvenuhskvienuhqgqvtwmkiwtwqveztwamuluhtwucmviwuhqvqvtwiwuhlepmuhtwlepmezuhqgvitwezpmdfuhskvienuhvitwezuhulkrpmtvenuhqvtwiwtvenxeuhtwletwezuhskentwle / tom_sawyer ✗ (hard_times)\n",
      "60000 20% (77m 31s) 2.4641 vipmmvlrtwleuhulenamuluhxepmuhqvdfpmleyptwleenypqvuhulvikrenlruhtwamuluhvitwamdfuhqgtwezpmypuhpmuluhqvendfuhtwezmvuluhtwamuluhqglepmuyuhskvienuhqgentwqvuhulenuhtwletwdfuhsatwamuluhsktwtwskviiguhraentw / dubliners ✗ (tale_of_two_cities)\n",
      "65000 21% (79m 47s) 2.5985 pmamdfuhdfpmviucuhpmuluhqgeztwamuluhskenamuhkrpmsauhxemvuhqgtwmkmvlruhulpmviuhkrpmsauhskiwkrpmdfuhqvskvikrpmgzuhxepmuhqvskvienqvkrpmamuluhxepmuhqvsktwleskvikramuhskvienuhqvskvienqvkrpmamuluhsavienezuh / alice_in_wonderland ✗ (oliver_twist)\n",
      "70000 23% (82m 26s) 2.5117 skiwkrpmdfuhultwamypenypuhtwskuhletwulultwiwuhenuhqgulqvengzuhqvleentwsauhlepmxeuhskkrpmiwypuhulenamuluhsatvuhsktwdfpmskenamqvletwmkpmuhvitwtwtvuhulpmviuhskenamuhskvienuhqgullekrpmeeuhtwamuluhamulmvdf / oliver_twist ✗ (tale_of_two_cities)\n",
      "75000 25% (84m 44s) 2.2655 qvendfuhqgqvtwqvqveniwlruhpmdfuluhskenamuhiguhamypmvamdfuhxepmuhqgsaenucpmguuhskiwpmuhxepmuhtwiwululpmtvuhenuhskvienuhskeniwenqvuhenuhskvienuhtwqvtwtwamypuhtwezpmqvuhamulmvdfuhqgqvmvamguuhravitwucypmv / tom_sawyer ✗ (dracula)\n",
      "80000 26% (87m 5s) 2.3078 skiwkrpmdfuhulenamuluhucenpmiwypuhsalemvenxeuhxepmuhullepmqvuhenuhqvenuhxeiwtwqvezmvamuhxepmuhulleengzuhulvienullepmgzezmvuhulenamuluhlrvimvqvkruhpkskentwamuhqvmvamuhucpmpmamqvuhsalelepmxzuhraleqjuhqg / oliver_twist ✗ (tale_of_two_cities)\n",
      "85000 28% (89m 28s) 2.3964 rasktwletwtvezkrviuhtwletwdfuhqvsaenskuhqvsaskpmtvtwezpmqvuhulenamuluhulvientwezuhulmvbhbhletwskskkramqvuhezpmguuhtwskenezuhskentwamuhqvsktwtvuhtwamuluhulenuhiwiwendfuhtwamuluhvimvuhamypulendfbhamulen / dubliners ✗ (tom_sawyer)\n",
      "90000 30% (91m 59s) 2.5373 vipmpmqvuhskvienuhqglrvimvgztwtwiwqvuhxepmuhsaiwucypmvkrszuhsktwlemvuluhletwultwmwuhulkrtvuhqgletwlrvipmiwuhulgztwiwqvuhtwmkenamuhskiwkrpmdfuhsatwamuluhsktwtwskviiguhrasaulmvlekryptwqvuhamulmvdfuhqvkr / hard_times ✗ (peter_pan)\n",
      "95000 31% (94m 16s) 2.4680 qgletwvipmmvulmvultwgzuhtwamuluhsktwskvikrpmlelekrqvuhqgulpmpmxeuhskvienuhtwqvlepmamuhqgskleenkrlruhtwamulnkuhpkucvimvleskuhlepmviuhskpmpmxeuhletwamulmvtwviuhqvendfuhulenamulbhbhlrvimvamultwezpmqvuhle / tom_sawyer ✗ (tale_of_two_cities)\n",
      "100000 33% (96m 47s) 2.5271 twamuluhxepmuhletwmkmvleskuhtwamuluhskvienuhsklemvamuluhskvienuhskvipmyptwqvuhsktwypeniwgzuhvitwtwtvuhskenamuhsatwamuluhpksaiwskmviwpmqvuhsktwamqvmvvimvxeuhskenamuhezentwuluhlemvtwamguuhraqvlepmulypmv / tom_sawyer ✗ (dubliners)\n",
      "105000 35% (99m 38s) 2.6395 qvqvtwviiwkrxeletwtwamypuhxepmuhlemvenuhvienuhamulmvdfuhtwletwamuhezpmlexeuhqvqvenmwuhratwmkpmiwuhsaezuhqgezleenuhlekrpmsauhtwezuhtwmkmvprnkuhraskenpmleuhtwulmvamdfuhtwamuluhletwmkpmuhdfpmskenamqvuhen / tom_sawyer ✗ (tale_of_two_cities)\n",
      "110000 36% (102m 16s) 2.6752 soraletwmktwuhlepmxeuhtwvipmuhletwtvezkrfqsouhuhraletwuliwpmdfuhraleqjuhsktwmvleypuhsooaenamuhoaenlpsouhqgskvitwmvlexeuhsaezuhqgletwamulmvtwviuhqvsouliguhuhravitwmktwqvuhletwtvezkrviuhsaenqvuhtwezpmqv / dubliners ✗ (oliver_twist)\n",
      "115000 38% (104m 38s) 1.9893 twamuluhpmuluhqvtwypvienleulvitwuhvitwmktwqvuhtwamuluhtwleenuhtwqvtwamguuhrasapmtvuhenuhqvenuhtwlrleeniwuhqvenuhucvikrleuluhdfpmiwiwpmamuhqvulmvuhvimvuhtwiwpmamuhenuhamulmvdfuhamypentwuhqgqvtwtwleuluh / peter_pan ✓\n",
      "120000 40% (106m 56s) 2.1275 qvtwvipmtvuhskvienuhpkqvtwmvletwiwmvkrguuhtwamuluhxepmuhtwypeniwengzuhtwamuluhvipmgzkruhsaiwulyptwlemvskuhulqvpmeziwenuhlrvimvvitwgzpmuhultwtwleulqvuhullepmamqvuhenuhqgqvmvleenmwuhvimvuhtwvimvamgzkren / dubliners ✗ (moby_dick)\n",
      "125000 41% (109m 26s) 2.5916 skvienuhqgsaendfuhtwamuluhxepmuhqvtwskmvqvuhamulpmtvuhvipmuhqvtwqvkrpmamuhqvsovitweztwiwulvitwlruhskvienuhqvvitwskleenlruhtwlrleeniwuhsavienezuhlrvimvqvqvengzuhjztwezmvuluhlrvipmiwuhenuhlepmxeuhvipmuh / tale_of_two_cities ✗ (oliver_twist)\n",
      "130000 43% (112m 9s) 2.4008 enuhlrvimviwiwkrgzuhtwletwdfuhiguhskvienuhulletwtvletwlpuhqgucviendfuhskvipmmwuhiwiwmvqjuhsktwqvqvengzuhiguhtwezmvuluhulqvlemvxeuhtwamguuhraamulmvlekquhpmuluhvidfpmskuhskvienuhqgiwpmpmmwuhtwamuluhvimv / hard_times ✗ (great_expectations)\n",
      "135000 45% (115m 0s) 2.0945 iwenqvqvpmiwpmypuhqglrvimvletwlrvimviwuhqglrvipmiwuhskuhraulmvuhsktwiwiwpmlevikruhtwlpuhraletwgzengzuhenuhvimvuhqvendfuhuliguhraamlrmvqvuhenuhamulmvdfuhulyptwbrtvpmuhqvmvamuluhgzkruhucpmpmuluhskvienuh / tom_sawyer ✓\n",
      "140000 46% (117m 38s) 2.8766 ezpmlexeuhleenxebhbhqgtwlrlepmxeuhtwamuluhulenuhtwezuhulxetwiwuhskenamuhtwamuhulenamguuhoatwezpmypuhletwmktwviuhskenamuhtwamuhulenamuluhyvuhvgraletwulultwiwuhsaezuhpmuluhtwiwtventwtwlelrenuhqgkrpmsauh / huckleberry_finn ✗ (great_expectations)\n",
      "145000 48% (120m 8s) 2.6252 raucletwiwypuhenuhsatvuhpmuluhsktwskvitwululenuhvitwtwtvuhqvenamuhuclepmdfuhtwvimvulkrpmleuhtwamuluhpmqvuhpkucypmvqvuhiwkrqvvipmeebhtwypmvnbuhtwamuluhskvienuhqgsaendfenuhqvmvuhiwkrqvvipmeeuhtwamguuhra / oliver_twist ✗ (dracula)\n",
      "150000 50% (122m 32s) 1.9540 vitwamokuhraqvvipmmvulqvtwkrszuhucqvenuhpmuluhsktwulviendfuhtwamqvuhskvieniwvimvenezuhtwamuluhvipmuhsktwmkmvleleenuhtwamqvuhulvitwezpmezuhtwamuluhezpmlexeuhxemvuhqvenuhqgucpmpmiwuhlrvimvlemvkrszvimvuh / peter_pan ✓\n",
      "155000 51% (125m 6s) 2.3434 skiwmvamypuhqvijletwiwiwpmleulqvuhtwamuluhvitwamdfuhulamlrmvviuhtwamuluhtwypvimvqvuhqgvimvenlrenuhpmuluhsktwvitwulxepmqvuhletwmktwviuhskenamuhtwamqvuhsaqvqvmvxyuhpklepmxeuhulmvxevikruhiwiwenuhxeiwtwqv / tom_sawyer ✗ (hard_times)\n",
      "160000 53% (127m 33s) 2.2627 vimvuhdftwlelruhqvqvenlelruhtwamuluhskvienuhqgqvsaendfamlrmvamuhtwucmviwuhqvulkrleuhskenamuhqvultwtwleulqvuhtwamuluhlepmxeuhqgsaulmvypuhtwamuluhulpmviuhqvendfuhulmvuhpkqvultwtwleulqvuhskvienuhqvtwqvkr / tale_of_two_cities ✗ (les_miserable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165000 55% (130m 11s) 2.3863 qgtwezpmqvtwvipmiwuhulmvuhskvikrpmxeuhtwamuhvitwamguuhralekrpmamuhvienuhsktwulmvendfuhskvienuhqgvimvenlrenuhvidfpmskuhulenqvuhucypkrlpuhpmxyuhralrvimvlegzqvuhtwamuluhsatvuhskvikrpmleenuhulvikramuhiwiw / peter_pan ✗ (tom_sawyer)\n",
      "170000 56% (132m 56s) 2.5907 salrletwvitwuhxepmuhiwiwkrxeuhqgvienezuhlrvikrpmsauhenuhqvmvuhtwlpuhratwypvitwskmvxevipmypuhtwiwtvmvqvqvpmgzuhsaletwmktwuhtwmkenamuhiguhezpmamdfuhvimvuhtwvipmuhqgtwulkrulmvulqvtvkrqvuhulvitwmvypmvxexe / dubliners ✗ (dracula)\n",
      "175000 58% (135m 58s) 2.7871 raulvitwdfuhtwpmcguhskvienuhqgtwypeniwgzuhqvmvamuhxepmuhulkrpmuhezmvamuhsktwvipmucyptwtvuhvieneztwiwulvitwlruhtwlrvienleulqvuhtwamguuhvgohvienezuhtwamuluhqvmvuhamypmvamokuhrasaletwlrleenprbhbhtwpmcguh / hard_times ✗ (great_expectations)\n",
      "180000 60% (138m 19s) 2.2532 letwamuhlrvimvqventwleypvimvuhqgvitwuluhucypkrleulqvuhucyppmiwypuhtwamguuhuhravipmskvipmxzuhxepmuhskvikqbhulqvtwokuhtwamuluhqvskleendfpmuluhqvskiwtwmvxeiwenulmvgzxyuhezpmlexeuhlrvimvucenezuhvimvuhqgsa / alice_in_wonderland ✗ (oliver_twist)\n",
      "185000 61% (140m 56s) 2.6193 ulmvuhulkrtvuhqgamqvmvvimvezmvskuhsaiwiwentwleuhskmvskuhtwvimvypmvsktwezuhtwamuluhulenamuluhskvikrpmxeuhtwamxyuhrasaiwtwvimvulqvtwskvieniwypuhtwiwululpmtvuhtwamuluhsktwamypulendfuhtwamqvuhqgezpmguuhqv / alice_in_wonderland ✗ (tom_sawyer)\n",
      "190000 63% (143m 14s) 2.6191 qvmvamuhpmuluheztwamuluhtwqvpmiwypqvmvskuhpmuluhqglepmlelepmamuhamulmvdfuhucypkrleulqvuhqgskvienuhqgqvtweztwamypqvuhqvijletwvipmqvmvlegzuhtwamuluhxepmuhtwlekrulenviuhtwamuluhulkrpmuhultwleletwxeuhpmul / tom_sawyer ✗ (tale_of_two_cities)\n",
      "195000 65% (145m 44s) 2.6111 raletwamulenxeuhletwamuhulenuhtwypvieniwlruhenuhamulmvdfuhskvienuhqgsaiwsktwulvimvpmgzuhvipmmvulqvtwkrszuhtwamuluhletwamuhsktwucqvenuhtwlpuhvgohiwiwenuhqvkruhlepmxeuhucentwgzqvuhsaenezuhiguhucvimvamul / tom_sawyer ✗ (tale_of_two_cities)\n",
      "200000 66% (148m 8s) 2.0221 enuhlrvimvucypmvucuhgziwtwamuhulpmviuhskiwkrpmypuhsatwamuluhqgletwultwmwuhqvenuhulvienlrtwiwtwuhpmqvuhsaiwleentwviuhulpmviuhtwletwdfuhsatwamguuhtwvilepmtvuhqvendfuhtwamuhsaiwtwulenmvsktwezezmvuhskvien / peter_pan ✓\n",
      "205000 68% (151m 8s) 2.2920 qvulenpmypuhlekrxeuhxepmuhulpmiwuhiwkrxeletwskvipmdfuhenuhamypkrqvuhtwezpmamuhulvitwqvuhqvenamuhskvienuhqgpmpmuluhlrvimvgzgzpmamqvuhvitwtwtvuhqvenamuhtwlpuhravimvenlrenuhskpmpmxeuhskpmpmlruhsavienuhul / alice_in_wonderland ✗ (dracula)\n",
      "210000 70% (153m 36s) 2.4276 qgulamlrmviwuhxepmuhsaenleuhpmviuhtwezenypuhqvdfpmskvimvdfuhucypeniwtvuhiwiwenuluhtwqvpmamdfuhezpmlexeuhqgtwiwulqvenypuhsktwvimvkrleuhulqvenmkuhenuhxepmuhskleensaullekrpmypuhtwamuluhvimvuhqvtwqvlepmam / peter_pan ✗ (dracula)\n",
      "215000 71% (156m 6s) 2.5728 qgulvitwypmvxemvvilrenezuhskvienuhqgskiwpmtvuhqgskvienlelruhtwletwdfuhulenamuluhqvlrvimvamuluhskmvskuhqvtwezmvultwezpmqvuhqgezmvamuhsktwulletwqvtwskuhletwmktwviuhamypmvamdfuhlemvenuhskmvskvienypuhskvi / oliver_twist ✗ (les_miserable)\n",
      "220000 73% (158m 26s) 2.4494 vimvuhqvtwmkiwtwqveztwamuluhlrvimvsksktwtveztwuhtwletwamuluhskvienuhqgqvtwvimviwuhamqvmviwlrvikquhtwamuluhvimvuhlrvimviwiwenxeuhqvtvezpmtvuhtwamuluhxepmuhulyptwxexetwuhiwiwenezqvuhtwamuluhpktwlrskmvle / hard_times ✗ (les_miserable)\n",
      "225000 75% (160m 51s) 2.3918 twamuluhviiguhraqvqvtwleskskenuhvipmmvulyptwiwtwuhvienuhxepmuhsagzpmypuhenuhlepmxeuhulgztwypcitwuhtwleentvuhtwletwdfuhezpmpmleuhtwamuluhxepmuhqviwiwendfuhtwamguuhralekrpmiwpmypuhiwkrxeletwtwamypuhqvul / tom_sawyer ✗ (dubliners)\n",
      "230000 76% (163m 9s) 1.8533 jzskpmpmlruhviezenskuhpmviuhqgskpmpmlruhpmviuhqvendfuhletwmvamqvenypuhtwamuluhqgpmfquhohtwypvienmkskenuhvienuhlepmxeuhsaiwtwulenmkmvlegzuhletwmvamqvenypuhtwamuluhucqvenuhtwamuhskiwkrpmeeuhrarararaezmv / dubliners ✓\n",
      "235000 78% (165m 47s) 2.8214 saiwtwskmvdfuhqvendfuhtwlpuhraqvtwypvienvimvxeuhqvmvamuhxepmuheztwiwtvpmlegzuhtwvimvxeuhtwamuluhsktwypvieniwentvuhtwamuhamypmvamdfuhvipmgzkruheniwiwtwletvezkruhiwenlepmezuhtwamuluhqvendfuhtwezenviuhvi / oliver_twist ✗ (dubliners)\n",
      "240000 80% (168m 22s) 2.6146 krpmsauhtwleenuhlrvipmiwuhdfpmamuhqgsaenxyuhuhohtwtwqvuhkrpmsauhulijvipmskuhqgtwiwskskmvleuhenuhqvijulmvuhqgulmvuhtwezeniwtvuhqgsaamoknkuhvgohlepmxeuhtwezuhucqvenuhkrpmsauhskmvskuhulenamdfuhqgqvendfuh / alice_in_wonderland ✗ (huckleberry_finn)\n",
      "245000 81% (171m 8s) 2.4197 iwenulvitwezmvulvitwqvuhtwmkmvulenvimvlrenezmvuhpmviuhqvijtwletwamuluhqgtwezuhulkrpmtvenuhtwskmvlegzuhsaiwmvezenxeuhpmviuhqvijtwletwamguuhraletwamulpmezskvienlelruhvitwucvikrleskuhsaezuhulgztwypcitwuh / peter_pan ✗ (hard_times)\n",
      "250000 83% (173m 45s) 2.7709 lemvenuhtwamuluhqgtwvipmleamuluhqvmvamuhvimvuhlrvimvskskpmviuhulenqvuhletwulqvenezuhtwamuluhqgsatvuhsktwulxemvleskuhlekrpmamuhtwiwpmamdfuhskuhucypkrulqvuhskenamuhskvienuhqgezpmulqvkrypuhqvendfuhulmvuh / huckleberry_finn ✗ (tom_sawyer)\n",
      "255000 85% (176m 26s) 2.8896 twamuluhqgvipmypentvuhxepmuhtwypmviwqvuhenuhqgulentwezuhxepmuhulmvtvuhenbhbhqglrvimvulentwuhqvendfuhtwamuhtwiwmvamdfuhiwdfpmtvuhqvmvamuhezpmlexeuhulqvengztwleuhqvmvamuhxepmuhulleengzuhulqvtwtvuhtwamul / alice_in_wonderland ✗ (les_miserable)\n",
      "260000 86% (178m 52s) 2.5632 enuhvitwiwpmulqvuhskenamuhsktwskvienlpbhsktwjeuhtwamuluhvivimvuyuhratwletwamuluhulmvuhlrvimvulultwlruhamulmvdfuhulkrpmuhxeiwtwqvezmvamuhvilepmdfuhulkrpmtvenuhskenamuhskvienuhqgvipmypentvuhxepmuhtwskmv / tale_of_two_cities ✗ (tom_sawyer)\n",
      "265000 88% (181m 31s) 2.0480 lrvimvpmlrbhenuhulijvimvenuhskvienuhqgulkrpmucpmpmiwuhtwamuluhvipmuhqvsaendfiwenuhqvijulenamuluhtwviyvuhqvijtwletwamuluhtwezuhlrvimvdfpmamqvuhqvendfuhdfpmviuhskvienuhqgezleenamuhpmviuhtwezuhtwvipmskuh / huckleberry_finn ✓\n",
      "270000 90% (183m 52s) 2.9435 eztwamuluhxepmuhtwvipmuhskvikrpmleenuhtwvimviwuhtwamuluhsktwqvqvengzuhiguhuhrapmuluhtwmvuluhpmuluhqvlrvimviwgzenqvuhtwiwululmviwuhulkrtvuhlrvimvamulsavienuhulijvileendfuhtwletwamuluhqgulqvenxeuhtwucen / tom_sawyer ✗ (huckleberry_finn)\n",
      "275000 91% (186m 41s) 2.6109 amenmvqvpmcguhtwlepmxetwtvuhqvezleenuhsaezuhvimvuhsktwmvskuhpmamdfuhqgletwamulpmezuhlepmpmgzuhsaezuhxepmuhsalepmeztwezuhtwamuluhulqvvimvenlrenuhiwenskvienypqvuhlekrpmsauhlepmxeuhskvienuhqgqvvipmmvulen / moby_dick ✗ (hard_times)\n",
      "280000 93% (189m 24s) 2.2802 twamuluhvipmuhqviwenmvulmvvimvuhletwamuhskenamuhulenamuluhucpmpmtvlrvipmqvuhskvikrpmtvbhletwamulentwiwuhskiwpmuhtwamuluhskvienulqvbhypmvqvkrezuhtwamuluhvimvuhtwypeniwgztwleuhpmuluhulvitwtvuhtwamqvuhqv / alice_in_wonderland ✗ (dubliners)\n",
      "285000 95% (191m 51s) 2.8781 enqvmvkrpmxzuhdfenqvuhtwamqvuhqglekrpmamuhpmuluhlekrpmamuhskvienuhsaenskuhpmuluhsaenskuhezpmlexeuhqgqvlemvenulqvuhtwqvpmamuluhvidfpmskuhskvienuhpkezpmululpmtvuhtwamuluhulenuhvimvkrleuhskvienuhtwezenam / tom_sawyer ✗ (hard_times)\n",
      "290000 96% (194m 22s) 2.2453 jzskmvenqvuhtwlpuhraulmvuhlepmxeuhsktwgzgzenleuhqvtwiwucypkrviucuhqvmvamuhulpmlruhskvienuhqgtwqvpmviuhsaletwmkuhqvulvikrenuhqvmvamuhletwskvikruhleenlrkrqvuhiwentwulqvuhpmuluhsktwmvleuluhtwlpuhraulqven / peter_pan ✗ (tom_sawyer)\n",
      "295000 98% (197m 1s) 2.3149 ulamlrmvleuhqvendfuhulmvbhbhulamlrmvletvuhvwqvendfvwuhulenamguuhuhraucleenezuhdfpmleleenbhtwkriwtvuhsaleenvimvlrenezmvuhulenamuluhamulmvdfuhsauctwtwamypuhskvienuhiwpmpmypuhpmqvuhulkrpmuhlrvimvezpmypuh / alice_in_wonderland ✗ (huckleberry_finn)\n",
      "300000 100% (199m 32s) 2.8502 ypmvpmletwamuhulqvpmezuhtwamuluhamulmvdfuhqvtwmkmvdfuhlemvtwamuluhxepmuhqvqvpmiwuhtwamuluhtwlepmtvuhqgpmpmuluhqgqvskvientvqvkrlpuhuhraeztwamuluhtvlekrulqvmvskuhpmuluhsktwvitwgzgzenamuhskenamuhletwmktw / les_miserable ✗ (oliver_twist)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 300000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbde29ed2b0>]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/.local/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/home/docker/.local/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        ''' Initialize the layers of this model.'''\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding layer that turns words into a vector of a specified size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # the LSTM takes embedded word vectors (of a specified size) as inputs \n",
    "        # and outputs hidden states of size hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # the linear layer that maps the hidden state output dimension \n",
    "        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        # initialize the hidden state (see code below)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        ''' At the start of training, we need to initialize a hidden state;\n",
    "           there will be none because the hidden state is formed based on perviously seen data.\n",
    "           So, this function defines a hidden state with all zeroes and of a specified size.'''\n",
    "        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        ''' Define the feedforward behavior of the model.'''\n",
    "        # create embedded word vectors for each word in a sentence\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        # get the output and hidden state by passing the lstm over our word embeddings\n",
    "        # the lstm takes in our embeddings and hiddent state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        # get the scores for the most likely tag for a word\n",
    "        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_outputs, dim=1)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "model = LSTM(EMBEDDING_DIM, HIDDEN_DIM, len(unique_pairs), len(all_categories))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfpmamuhoasaucqvuhskvienuhlemvenuhxepmuhskpmpmamskiwmvamypuhultwtwdfxyuhoaqvkruhskvikrpmleuhiwiwenuhypmviwpmlexeuhulenamuluhqvtwlekrulentwleypuhsktwlrvimvdfuhtwiwtvmvqvmvmkviiguhoatwlekronenuhtwamuluh torch.Size([200])\n",
      "torch.Size([12])\n",
      "moby_dick\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lineToIndex(line):\n",
    "    tensor = torch.zeros(len(line), dtype=torch.long)\n",
    "    for li, pair in enumerate(lineToPairs(line)):\n",
    "        tensor[li] = pairToIndex(pair)\n",
    "    return tensor\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor(all_categories.index(category), dtype=torch.long)\n",
    "    line_tensor = lineToIndex(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    category, line, category_tensor, inputs = randomTrainingExample()\n",
    "    print(line, inputs.shape)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores[-1].shape)\n",
    "    print(category)\n",
    "    print(category_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-00390a29da17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "for setps in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    _, _, category_tensor, inputs = randomTrainingExample()\n",
    "\n",
    "    # Step 1. Remember that Pytorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Step 3. Run our forward pass.\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "    #  calling optimizer.step()\n",
    "    # loss = loss_function(\n",
    "    #     tag_scores, category_tensor.repeat(tag_scores.shape[0])\n",
    "    # )\n",
    "    loss = loss_function(tag_scores[-1].view(1, -1), category_tensor.view(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even with .backwards(retain_graph=True) the same error appears.\n",
    "# couldn't find a fix so far, hence not able to continue with lstms\n",
    "# for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level CNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_data = pd.DataFrame(\n",
    "    {\n",
    "        'lables': y_train.astype(str), 'text': X_train\n",
    "    }\n",
    ")\n",
    "valid_file_data = pd.DataFrame(\n",
    "    {\n",
    "        'lables': y_valid.astype(str), 'text': X_valid\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_data.to_csv(\"../data/train.csv\", index=False, header=False)\n",
    "valid_file_data.to_csv(\"../data/valid.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 21M\r\n",
      "-rw-r--r-- 1 docker docker 4.3M Sep 24 13:04 train.csv\r\n",
      "-rw-r--r-- 1 docker docker 2.1M Sep 24 13:04 valid.csv\r\n",
      "-rw-r--r-- 1 docker docker 1.2M Jul 19  2017 xtest_obfuscated.txt\r\n",
      "-rw-r--r-- 1 docker docker  13M Jul 19  2017 xtrain_obfuscated.txt\r\n",
      "-rw-r--r-- 1 docker docker  68K Jul 19  2017 ytrain.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to run the char-level cnn clone the repo `github.com/cmayrhofer/char-cnn-text-classification-pytorch` (a modified fork of `github.com/srviest/char-cnn-text-classification-pytorch` implementing `https://arxiv.org/pdf/1509.01626.pdf`) and execute the training with the following commands:\n",
    "\n",
    "```\n",
    "git clone git@github.com:cmayrhofer/char-cnn-text-classification-pytorch.git\n",
    "\n",
    "cd har-cnn-text-classification-pytorch\n",
    "\n",
    "python train.py --train_path ../data/train.csv --val_path ../data/valid.csv --alphabet_path alphabet_a-z.json\n",
    "```\n",
    "\n",
    "To run the predictions on the file:\n",
    "\n",
    "```\n",
    "python predict.py --val_path ../data/xtest_obfuscated.txt --alphabet_path alphabet_a-z.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Bag Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_line_length = None\n",
    "X = np.array([line[:max_line_length] for line in train_data])\n",
    "y = train_labels\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "# if not os.path.isdir('./.data'):\n",
    "#     os.mkdir('./.data')\n",
    "# train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "#     root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "BATCH_SIZE = 16*2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineToIndices(line):\n",
    "    pairs = [line[i:i + 2] for i, _ in enumerate(line) if i % 2 == 0]\n",
    "    return [pairToIndex(pair) for pair in pairs]\n",
    "\n",
    "\n",
    "def dataset_gen(X, y):\n",
    "    return [\n",
    "        (\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "            torch.tensor(lineToIndices(line))\n",
    "        ) for label, line in zip(y, X)\n",
    "    ]\n",
    "    \n",
    "\n",
    "train_dataset = dataset_gen(X_train, y_train)\n",
    "valid_dataset = dataset_gen(X_valid, y_valid)\n",
    "test_dataset = [\n",
    "    (torch.tensor(0, dtype=torch.long), torch.tensor(lineToIndices(line))) for line in test_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextAuthor(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "class TextAuthorPlusHidden(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = self.relu(self.fc1(embedded))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(unique_pairs)\n",
    "EMBED_DIM = 64\n",
    "NUN_CLASS = len(class_distribution)\n",
    "# model = TextAuthor(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n",
    "model = TextAuthorPlusHidden(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)\n",
    "\n",
    "def predict(data_):\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    scores, labels = list(), list()\n",
    "    for text, offsets, _ in data:\n",
    "        text, offsets = text.to(device), offsets.to(device)\n",
    "        with torch.no_grad():\n",
    "            output_ = torch.exp(model(text, offsets)).topk(5)\n",
    "        scores.append(output_[0].numpy())\n",
    "        labels.append(output_[1].numpy())\n",
    "    return np.vstack(scores), np.vstack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0448(train)\t|\tAcc: 50.9%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 49.4%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0426(train)\t|\tAcc: 52.6%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 52.0%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0424(train)\t|\tAcc: 52.7%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 51.7%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0412(train)\t|\tAcc: 53.7%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 46.5%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0408(train)\t|\tAcc: 54.6%(train)\n",
      "\tLoss: 0.0005(valid)\t|\tAcc: 52.3%(valid)\n",
      "Epoch: 6  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0410(train)\t|\tAcc: 54.7%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 52.5%(valid)\n",
      "Epoch: 7  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0401(train)\t|\tAcc: 55.5%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 51.3%(valid)\n",
      "Epoch: 8  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0399(train)\t|\tAcc: 55.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 54.8%(valid)\n",
      "Epoch: 9  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0394(train)\t|\tAcc: 55.8%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 53.1%(valid)\n",
      "Epoch: 10  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0391(train)\t|\tAcc: 56.1%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 51.1%(valid)\n",
      "Epoch: 11  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0388(train)\t|\tAcc: 56.3%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 52.4%(valid)\n",
      "Epoch: 12  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0387(train)\t|\tAcc: 56.6%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 54.2%(valid)\n",
      "Epoch: 13  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0385(train)\t|\tAcc: 57.2%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 54.9%(valid)\n",
      "Epoch: 14  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0383(train)\t|\tAcc: 57.0%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 53.4%(valid)\n",
      "Epoch: 15  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0380(train)\t|\tAcc: 57.5%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 55.3%(valid)\n",
      "Epoch: 16  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0380(train)\t|\tAcc: 57.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 54.7%(valid)\n",
      "Epoch: 17  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0376(train)\t|\tAcc: 57.7%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 51.7%(valid)\n",
      "Epoch: 18  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0375(train)\t|\tAcc: 57.8%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 54.9%(valid)\n",
      "Epoch: 19  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0373(train)\t|\tAcc: 57.9%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.3%(valid)\n",
      "Epoch: 20  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0371(train)\t|\tAcc: 58.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 53.7%(valid)\n",
      "Epoch: 21  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0369(train)\t|\tAcc: 58.3%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 54.0%(valid)\n",
      "Epoch: 22  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0368(train)\t|\tAcc: 58.5%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 49.5%(valid)\n",
      "Epoch: 23  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0366(train)\t|\tAcc: 58.8%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 54.6%(valid)\n",
      "Epoch: 24  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0364(train)\t|\tAcc: 59.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 53.3%(valid)\n",
      "Epoch: 25  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0363(train)\t|\tAcc: 59.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.7%(valid)\n",
      "Epoch: 26  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0362(train)\t|\tAcc: 59.2%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.6%(valid)\n",
      "Epoch: 27  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0361(train)\t|\tAcc: 59.2%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 53.0%(valid)\n",
      "Epoch: 28  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0359(train)\t|\tAcc: 59.7%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 53.2%(valid)\n",
      "Epoch: 29  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0358(train)\t|\tAcc: 59.9%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 51.3%(valid)\n",
      "Epoch: 30  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0357(train)\t|\tAcc: 60.1%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.1%(valid)\n",
      "Epoch: 31  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0355(train)\t|\tAcc: 59.8%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.1%(valid)\n",
      "Epoch: 32  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0355(train)\t|\tAcc: 60.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.2%(valid)\n",
      "Epoch: 33  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0354(train)\t|\tAcc: 60.1%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 54.8%(valid)\n",
      "Epoch: 34  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0352(train)\t|\tAcc: 60.2%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 54.9%(valid)\n",
      "Epoch: 35  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0352(train)\t|\tAcc: 60.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.7%(valid)\n",
      "Epoch: 36  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0350(train)\t|\tAcc: 60.9%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.0%(valid)\n",
      "Epoch: 37  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0350(train)\t|\tAcc: 61.0%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 53.7%(valid)\n",
      "Epoch: 38  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0349(train)\t|\tAcc: 60.9%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.0%(valid)\n",
      "Epoch: 39  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0347(train)\t|\tAcc: 61.1%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.9%(valid)\n",
      "Epoch: 40  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0347(train)\t|\tAcc: 61.0%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.4%(valid)\n",
      "Epoch: 41  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0346(train)\t|\tAcc: 61.0%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 54.7%(valid)\n",
      "Epoch: 42  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0345(train)\t|\tAcc: 61.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.5%(valid)\n",
      "Epoch: 43  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0344(train)\t|\tAcc: 61.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.8%(valid)\n",
      "Epoch: 44  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0343(train)\t|\tAcc: 61.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.7%(valid)\n",
      "Epoch: 45  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0343(train)\t|\tAcc: 61.5%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.2%(valid)\n",
      "Epoch: 46  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0342(train)\t|\tAcc: 61.8%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 54.9%(valid)\n",
      "Epoch: 47  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0340(train)\t|\tAcc: 62.0%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.9%(valid)\n",
      "Epoch: 48  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0341(train)\t|\tAcc: 61.6%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.8%(valid)\n",
      "Epoch: 49  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0341(train)\t|\tAcc: 61.7%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.9%(valid)\n",
      "Epoch: 50  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0340(train)\t|\tAcc: 62.0%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 57.1%(valid)\n",
      "Epoch: 51  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0339(train)\t|\tAcc: 62.0%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.8%(valid)\n",
      "Epoch: 52  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0338(train)\t|\tAcc: 62.2%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 55.3%(valid)\n",
      "Epoch: 53  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0338(train)\t|\tAcc: 62.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 57.1%(valid)\n",
      "Epoch: 54  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0337(train)\t|\tAcc: 62.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.6%(valid)\n",
      "Epoch: 55  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0337(train)\t|\tAcc: 62.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.8%(valid)\n",
      "Epoch: 56  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0336(train)\t|\tAcc: 62.4%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 54.3%(valid)\n",
      "Epoch: 57  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0335(train)\t|\tAcc: 62.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.5%(valid)\n",
      "Epoch: 58  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0334(train)\t|\tAcc: 62.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 57.1%(valid)\n",
      "Epoch: 59  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0334(train)\t|\tAcc: 62.6%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.1%(valid)\n",
      "Epoch: 60  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0334(train)\t|\tAcc: 62.6%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.9%(valid)\n",
      "Epoch: 61  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0333(train)\t|\tAcc: 62.7%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.2%(valid)\n",
      "Epoch: 62  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0333(train)\t|\tAcc: 62.8%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 57.3%(valid)\n",
      "Epoch: 63  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0332(train)\t|\tAcc: 62.9%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.3%(valid)\n",
      "Epoch: 64  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0332(train)\t|\tAcc: 62.8%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 55.9%(valid)\n",
      "Epoch: 65  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0332(train)\t|\tAcc: 62.8%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.9%(valid)\n",
      "Epoch: 66  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0331(train)\t|\tAcc: 63.1%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.6%(valid)\n",
      "Epoch: 67  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0330(train)\t|\tAcc: 62.9%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.6%(valid)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0330(train)\t|\tAcc: 63.1%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 57.2%(valid)\n",
      "Epoch: 69  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0330(train)\t|\tAcc: 63.2%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.7%(valid)\n",
      "Epoch: 70  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0329(train)\t|\tAcc: 63.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.4%(valid)\n",
      "Epoch: 71  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0329(train)\t|\tAcc: 63.2%(train)\n",
      "\tLoss: 0.0004(valid)\t|\tAcc: 56.8%(valid)\n",
      "Epoch: 72  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0329(train)\t|\tAcc: 63.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 57.3%(valid)\n",
      "Epoch: 73  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0328(train)\t|\tAcc: 63.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.4%(valid)\n",
      "Epoch: 74  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0328(train)\t|\tAcc: 63.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.7%(valid)\n",
      "Epoch: 75  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0328(train)\t|\tAcc: 63.5%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 57.2%(valid)\n",
      "Epoch: 76  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0327(train)\t|\tAcc: 63.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 57.2%(valid)\n",
      "Epoch: 77  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0327(train)\t|\tAcc: 63.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.9%(valid)\n",
      "Epoch: 78  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0327(train)\t|\tAcc: 63.5%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.6%(valid)\n",
      "Epoch: 79  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0326(train)\t|\tAcc: 63.6%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.5%(valid)\n",
      "Epoch: 80  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0326(train)\t|\tAcc: 63.8%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 56.9%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 80\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97)\n",
    "\n",
    "sub_train_, sub_valid_ = train_dataset, valid_dataset\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  7],\n",
       "       [ 1,  5],\n",
       "       [10,  7],\n",
       "       [ 8,  6],\n",
       "       [10,  8],\n",
       "       [ 7,  4],\n",
       "       [ 6,  1],\n",
       "       [ 6,  1],\n",
       "       [10,  7],\n",
       "       [ 8,  1]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(test_dataset)[1][:10, :2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_pairs(line: str) -> list:\n",
    "    return [line[i:i + 2] for i in range(0, len(line), 2)]\n",
    "\n",
    "def write_to_file(lines, file_name: str):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for line in lines:\n",
    "            file.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_data = pd.DataFrame(\n",
    "    {\n",
    "        'labels': y_train.astype(str), 'text': X_train\n",
    "    }\n",
    ")\n",
    "valid_file_data = pd.DataFrame(\n",
    "    {\n",
    "        'labels': y_valid.astype(str), 'text': X_valid\n",
    "    }\n",
    ")\n",
    "\n",
    "train_file_data['labels'].replace('([0-9]+)', '__label__\\\\1 ', regex=True, inplace=True)\n",
    "train_file_data['text'] = pd.Series([' '.join(line_to_pairs(line)) for line in train_file_data['text'].values])\n",
    "\n",
    "valid_file_data['labels'].replace('([0-9]+)', '__label__\\\\1 ', regex=True, inplace=True)\n",
    "valid_file_data['text'] = pd.Series([' '.join(line_to_pairs(line)) for line in valid_file_data['text'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_data['data'] = train_file_data['labels'] + train_file_data['text']\n",
    "valid_file_data['data'] = valid_file_data['labels'] + valid_file_data['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(train_file_data['data'].values, \"../data/train_ft.txt\")\n",
    "write_to_file(valid_file_data['data'].values, \"../data/valid_ft.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40M\r\n",
      "-rw-r--r-- 1 docker docker 4.3M Sep 24 13:04 train.csv\r\n",
      "-rw-r--r-- 1 docker docker  16M Sep 30 19:36 train_ft.txt\r\n",
      "-rw-r--r-- 1 docker docker 2.1M Sep 24 13:04 valid.csv\r\n",
      "-rw-r--r-- 1 docker docker 4.0M Sep 30 19:36 valid_ft.txt\r\n",
      "-rw-r--r-- 1 docker docker 1.2M Jul 19  2017 xtest_obfuscated.txt\r\n",
      "-rw-r--r-- 1 docker docker  13M Jul 19  2017 xtrain_obfuscated.txt\r\n",
      "-rw-r--r-- 1 docker docker 6.2K Sep 25 12:53 ytest.txt\r\n",
      "-rw-r--r-- 1 docker docker  68K Jul 19  2017 ytrain.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__7 oh sk mv en qv uh tw am uh ul en am ul uh qv ij ul en am ok uh ra qv le tw am ul en xe uh iw en ul mv mk uh xe pm uh xe xe kr ul qv uh iw en ul mv mk uh ul qv pm ez uh tw am ul uh xe pm uh tw sk en ez uh vi tw ez uh xe pm uh qv tw mk mv iw uh tw am ul uh ul qv en iw ul kr pm uh qv tw tw le ul uh sk en tw sk uh xe pm uh tw sk en ez uh qv gz mv am qv uh tw ez pm qv uh vi df pm vi uc uh tw mk ij ig uh sk vi en uh pk ul mv uh am ul mv df uh tw le en gz ez pm yp uh ul ij vi en yp uh ig uh ra sa en df uh sa le tw mk tw uh am qv tw iw xe uh tw mk mv iw uh sa ez uh xe pm uh le tw ul ul tw tv uh tw am ul\r\n",
      "__label__10 sk vi en uh qg ul mv uh xe xe pm uh ul qv kr sk uh tw am ul uh lr vi mv df pm iw tv uh sk vi en uh qg xe tw mv am yp le tw uc sk vi en am uh le tw am uh xe pm uh qv sk vi tw uh tw am ul uh am ul mv df uh gz kr uh ul mv uh lr vi mv am yp kr pm ul uh sk vi en uh qg qv le mv en ul qv bh gz kr uh tw ez en yp uh tw am qv uh vi tw am df uh ul tw vi vi pm tv uh qv ij lr vi mv iw le en sk uh le tw am uh xe xe pm uh lr vi mv uc en ul uh qg ez mv le lr uh sk vi en uh qg sk tw le uh sk vi en uh qg sk iw mv df uh ul mv tw tv iw en uh qg ul am lr mv qv uh ul vi en qv en tw iw gz uh en uh qv en df uh qv qv pm le mw uh qv qv mv qj\r\n",
      "__label__10 tw am uh tw iw mv am df uh sa iw vi pm uh qg sk vi en am uh ul xe tw iw uh qv mv am uh am ul mv df uh le tw iw xe xe kr ez uh qv mv am ul uh sk tw mk pm ez uh tw am uh qg uc vi mv le sk uh le pm xe uh sk tw gz gz pm ul qv uh tw am uh vi tw am ok uh ra qv tw tw vi uc uh qv ij le tw le en tw df uh tw am ul uh pm ul uh sa iw le en tw vi uh sk tw sk vi tw yp qv tw sk uh am yp mv am df uh qg ul en pm le am ul uh sk vi en uh vi mv am yp uh tw am ul uh le pm xe uh le tw iw xe xe kr ez uh ul en tw le lr uh en uh le tw mk pm uh sk vi en uh qg vi pm pm ul ul mv gz qv uh sk tw le tw vi le pm yp bh tw tw le am ul uh en\r\n",
      "__label__6 ul pm vi uh qv en df uh am yp mv am df uh qg le en tw ul uh en uh lr vi kr am uh am yp mv am df uh vi mv uh qg qv tw sa tw uh ul en tw le lr uh le tw am uh sk tw qv mv en le uh sk iw mv am yp uh tw am gu uh ds ra qv tw mk iw tw qv ez tw am ul uh tw qv kr ez en uh sa tw am ul uh pk sa en iw gz uh sa tw am gu uh ra qv le mv en xe xe en uh xe pm uh iw iw kr xe uh iw iw en uh qg ez tw am ul uh vi mv uh sk iw pm lr uh am ul mv df uh qv lr vi mv am ul uh pk qv iw iw pm sk uh iw kr xe mv ul kr en tw tv uh tw mk en am uh sa tw am ul ds uh qg sk iw mv am yp uh tw am ul uh sk mv en qv uh ds oa am yv ds\r\n"
     ]
    }
   ],
   "source": [
    "! head -n4 ../data/valid_ft.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('../data/train_ft.txt', ws=30, wordNgrams=1, verbose=5, loss='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t6503\n",
      "P@1\t0.408\n",
      "R@1\t0.408\n"
     ]
    }
   ],
   "source": [
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "\n",
    "print_results(*model.test('../data/valid_ft.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
